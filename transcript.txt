 《Penguin Random House Audio》 Penguin Random House Audio presents Why Machines Learn The Elegant Math Behind Modern AI by Anil Ananthaswamy Read for you by Rene Ruiz Publishers' Note This audiobook contains a bonus PDF of equations, graphs, and illustrations. To teachers everywhere, sung and unsung. Whatever we do, we have to make our life vectors, lines with force and direction.

Liam Neeson as FBI agent Mark Felt in the 2017 movie of the same name. The author acknowledges with gratitude the support of the Alfred P. Sloan Foundation in the research and writing of this book.

Prologue Buried on page 25 of the July 8, 1958 issue of The New York Times was a rather extraordinary story. The headline read, New Navy Device Learns by Doing. Psychologists shows embryo of computer designed to read and grow wiser.

The opening paragraph raised the stakes. The Navy revealed the embryo of an electronic computer today that it expects will be able to walk, talk, see, write, reproduce itself, and be conscious of its existence. With hindsight, the hyperbole is obvious and embarrassing, but The New York Times wasn't entirely at fault.

Some of the over-the-top talk also came from Frank Rosenblatt, a Cornell University psychologist and project engineer. Rosenblatt, with funding from the U.S. Office of Naval Research, had invented the Perceptron, a version of which was presented at a press conference the day before The New York Times' story about it appeared in print.

According to Rosenblatt, the Perceptron would be the first device to think as the human brain, and such machines might even be sent to other planets as mechanical space explorers. None of this happened. The Perceptron never lived up to the hype.

Nonetheless, Rosenblatt's work was seminal. Almost every lecturer on artificial intelligence, AI, today, will harken back to the Perceptron, and that's justified. This moment in history, the arrival of large language models, LLMs, such as ChatGPT, and its ilk and our response to it, which some have likened to what it must have felt like in the 1910s and 20s, when physicists were confronted with the craziness of quantum mechanics, has its roots in research initiated by Rosenblatt.

There's a line in the New York Times story that only hints at the revolution the Perceptron said in motion. Dr. Rosenblatt said he could explain why the machine learned only in highly technical terms.

The story, however, had none of the highly technical details.

This book does. It tackles the technical details. It explains the elegant mathematics and algorithms that have, for decades, energized and excited researchers in machine learning, a type of AI that involves building machines that can learn to discern patterns in data without being explicitly programmed to do so.

Trained machines can then detect similar patterns in new, previously unseen data, making possible applications that range from recognizing pictures of cats and dogs to creating, potentially, autonomous cars and other technology. Machines can learn because of the extraordinary confluence of math and computer science, with more than a dash of physics and neuroscience added to the mix. Machine learning, ML, is a vast field populated by algorithms that leverage relatively simple math that goes back centuries.

Math one learns in high school or early in college. There's, of course, elementary algebra. Another extremely important cornerstone of machine learning is calculus, co-invented by no less a polymath than Isaac Newton.

The field also relies heavily on the work of Thomas Bayes, the 18th century English statistician and minister, who gave us the eponymous Bayes' theorem, a key contribution to the field of probability and statistics. The work of German mathematician Karl Friedrich Gauss on the Gaussian distribution and the bell-shaped curve also permeates machine learning. Then there's linear algebra, which forms the backbone of machine learning.

The earliest exposition of this branch of mathematics appears in a 2,000-year-old Chinese text, nine chapters on the mathematical art.

The modern version of linear algebra has its roots in the work of many mathematicians, but mainly Gauss, Gottfried Wilhelm Leibniz, Wilhelm Jordan, Gabriel Kramer, Hermann Gunther Grossmann, James Joseph Sylvester, and Arthur Cayley. By the mid-1850s, some of the basic math that would prove necessary to building learning machines was in place, even as other mathematicians continued developing more relevant mathematics and birthed and advanced the field of computer science. Yet few could have dreamed that such early mathematical work would be the basis for the astounding developments in AI over the past half-century, particularly over the last decade, some of which may legitimately allow us to envision the resemblance of the kind of future Rosenblatt was over-optimistically foreshadowing in the 1950s.

This book tells the story of this journey, from Rosenblatt's perceptron to modern-day deep neural networks, elaborate networks of computational units called artificial neurons, through the lens of key mathematical ideas underpinning the field of machine learning. It eases gently into the math and then, ever so slowly, ratchets up the difficulty. As we go from the relatively simple ideas of the 1950s to the somewhat more involved math and algorithms that power today's machine learning systems.

Hence, we will unabashedly embrace equations and concepts from at least four major fields of mathematics, linear algebra, calculus, probability and statistics, and optimization theory, to acquire the minimum theoretical and conceptual knowledge necessary to appreciate the awesome power we are bestowing on machines. It is only when we understand the inevitability of learning machines that we will be prepared to tackle a future in which AI is ubiquitous for good and for bad. Getting under the mathematical skin of machine learning is crucial to our understanding of not just the power of the technology, but also its limitations.

Machine learning systems are already making life-altering decisions for us, approving credit card applications and mortgage loans, determining whether a tumor is cancerous, predicting the prognosis for someone in cognitive decline, will they go on to get Alzheimer's, and deciding whether to grant someone bail. Machine learning has permeated science too. It is influencing chemistry, biology, physics, and everything in between.

It's being used in the study of genomes, extrasolar planets, the intricacies of quantum systems and much more, and as of this writing, the world of AI is a buzz with the advent of large language models such as chatGPT. The ball has only just gotten rolling. We cannot leave decisions about how AI will be built and deployed solely to its practitioners.

If we are to effectively regulate this extremely useful, but disruptive, and potentially threatening technology, another layer of society, educators, politicians, policymakers, science communicators, or even interested consumers of AI, must come to grips with the basics of the mathematics of machine learning. In her book, Is Math Real? Mathematician Eugenia Cheng writes about the gradual process of learning mathematics. It can seem like we're taking very small steps and not getting anywhere, before suddenly we look behind us and discover we've climbed a giant mountain.

All these things can be disconcerting, but accepting a little intellectual discomfort, or sometimes a lot of it, is an important part of making progress in math.

Fortunately, the intellectual discomfort in store for us is eminently durable and more than assuaged by the intellectual payoff. Because underlying modern ML is some relative...